{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.spatial import procrustes\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from l2gx.align.utils import to_device\n",
    "import networkx as nx\n",
    "from scipy.stats import special_ortho_group\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "# Suppress common UMAP and sklearn warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, SpectralClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåå <font color=\"grey\"> Local2Global X - Graph Representation Learning at Scale</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"grey\">  Table of Contents</font>\n",
    "\n",
    "üèóÔ∏è <a href='#chapter1'>Structure</a>\n",
    "\n",
    "üìä <a href='#chapter2'>Datasets</a>\n",
    "\n",
    "üåê <a href='#chapter3'>Graphs</a>\n",
    "\n",
    "üß© <a href='#chapter4'>Patches</a>\n",
    "\n",
    "üéØ <a href='#chapter5'>Embedding</a>\n",
    "\n",
    "üîó <a href='#chapter6'>Alignment</a>\n",
    "\n",
    "üå≥ <a href='#chapter7'>Hierarchical alignment</a>\n",
    "\n",
    "üìà <a href='#chapter8'>Visualisation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter1'> üèóÔ∏è <font color=\"grey\">Structure </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five main parts to the package, organised as follows.\n",
    "\n",
    "```\n",
    "l2gv2/\n",
    "‚îú‚îÄ‚îÄ datasets/\n",
    "‚îú‚îÄ‚îÄ graphs/\n",
    "‚îú‚îÄ‚îÄ patch/\n",
    "‚îú‚îÄ‚îÄ embedding/\n",
    "‚îî‚îÄ‚îÄ align/\n",
    "    ‚îú‚îÄ‚îÄ l2g/\n",
    "    ‚îî‚îÄ‚îÄ geo/\n",
    "```\n",
    "\n",
    "A brief overview of the contents:\n",
    "\n",
    "* ```datasets``` contains interfaces are provided for various common benchmark datasets. \n",
    "* ```graphs``` contains wrappers for graphs represented as lists of edges in pytorch-geometric ```data.edge_index``` format. These implemented features such as fast adjacency look-up and a variety of algorithms on graphs.\n",
    "* ```patch``` directory contains datastructures to represent patches and patch graphs, as well as methods to subdivide a graph into patches. \n",
    "* ```embedding``` contains various graph embedding methods, including Graph Autoencoders (GAE) and [Variational Graph Autoencoders (VGAE)](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.VGAE.html).\n",
    "* ```align``` contains two methods to compute the alignment of patches into a single graph embedding: eigenvalue synchronisation based on the [Local2Global](https://link.springer.com/article/10.1007/s10994-022-06285-7) algorithm, and the new method based on learning the alignment using a one-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter2'> üìä <font color=\"grey\">Datasets </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2Gv2 framework provides access to multiple graph datasets spanning different domains and scales. All datasets are accessible through the unified `get_dataset()` interface and support conversion between multiple formats (PyTorch Geometric, Raphtory, Polars).\n",
    "\n",
    "| Dataset | Type | Nodes | Edges | Features | Domain |\n",
    "|---------|------|-------|-------|----------|--------|\n",
    "| **Cora** | Static Citation | 2,708 | 10,556 | 1,433 | üìö Academic Papers |\n",
    "| **AS-733** | Temporal Network | 7,716 | 45,645 | Temporal | üåê Internet Infrastructure |\n",
    "| **DGraph** | Financial | ~3M | ~4M | Multiple | üí∞ Fraud Detection |\n",
    "| **Elliptic** | Bitcoin | 203,769 | 234,355 | 166 | ‚Çø Cryptocurrency |\n",
    "| **MAG240M** | Academic | 244M+ | 1.7B+ | Rich | üéì Citation Graph |\n",
    "| **ORBITAAL** | Bitcoin Temporal | 252M (1K sample) | 785M (5K sample) | Temporal + Anomaly | ‚Çø Financial Fraud |\n",
    "\n",
    "#### Dataset Details\n",
    "\n",
    "* **Cora**: The [Cora dataset](https://graphsandnetworks.com/the-cora-dataset/) is a citation network of 2,708 scientific publications divided into 7 classes. Each node has a 1,433-dimensional feature vector indicating word presence/absence. Accessed through PyTorch Geometric's [Planetoid](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset.\n",
    "\n",
    "* **AS-733**: The [SNAP autonomous systems AS-733](https://snap.stanford.edu/data/as-733.html) dataset contains 733 daily snapshots spanning 785 days (November 1997 to January 2000). Nodes represent autonomous systems and edges indicate communication events.\n",
    "\n",
    "* **DGraph**: [DGraph](https://dgraph.xinye.com/dataset) is a real-world financial graph for anomaly detection research. Described in [DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection](https://arxiv.org/abs/2207.03579). Requires manual download.\n",
    "\n",
    "* **Elliptic**: The [Elliptic dataset](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set) maps Bitcoin transactions to licit/illicit categories. Contains 203,769 transactions with 166 features each. Used in [Anti-Money Laundering in Bitcoin](https://arxiv.org/pdf/1908.02591) research. Requires manual download from Kaggle.\n",
    "\n",
    "* **MAG240M**: The [MAG240M](https://ogb.stanford.edu/docs/lsc/mag240m/) dataset is a large heterogeneous academic citation graph with 244+ million nodes (papers, authors, institutions, fields) and 1.7+ billion edges. Requires the OGB library and substantial storage (~100GB).\n",
    "\n",
    "* **ORBITAAL**: The [ORBITAAL](https://www.nature.com/articles/s41597-025-04595-8) dataset is a comprehensive temporal Bitcoin transaction graph covering 13 years (2009-2021) with 252M entities and 785M transactions. Features timestamped transactions, entity types (exchanges, wallets, services, miners), and anomaly labels for financial fraud detection. Ideal for temporal graph neural networks and cryptocurrency flow analysis.\n",
    "\n",
    "\n",
    "For the datasets requiring manual download, provide the path:\n",
    "```\n",
    "elliptic = get_dataset(\"Elliptic\", source_file=\"/path/to/elliptic.zip\")\n",
    "dgraph = get_dataset(\"DGraph\", source_file=\"/path/to/dgraph.zip\")\n",
    "```\n",
    "\n",
    "All datasets support conversion to different formats and follow the PyTorch Geometric convention. Temporal graphs return iterables over time slices, and graphs can be exported to Raphtory or NetworkX formats for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current datasets: ['as-733', 'Cora', 'DGraph', 'Elliptic', 'MAG240M', 'ORBITAAL']\n"
     ]
    }
   ],
   "source": [
    "from l2gx.datasets import get_dataset, list_available_datasets\n",
    "datasets = list_available_datasets()\n",
    "print(f\"Current datasets: {datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge and node data from memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dc193e667249e883706824798d8088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=10556), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b5c24892a48098325456635e5a3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), IntProgress(value=0, max=2708), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge and node data from memory\n",
      "Data(edge_index=[2, 10556], num_nodes=2708, timestamp=0, x=[2708, 1434])\n"
     ]
    }
   ],
   "source": [
    "cora = get_dataset(\"Cora\")\n",
    "tg = cora.to(\"torch-geometric\")\n",
    "print(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 2708 nodes, 10556 edges\n",
      "Labels: 7 unique classes\n"
     ]
    }
   ],
   "source": [
    "G = cora.to(\"raphtory\").to_networkx()\n",
    "labels = tg.y.numpy()\n",
    "print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"Labels: {len(np.unique(labels))} unique classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter3'> üåê <font color=\"grey\">Graphs </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three wrappers for graphs that were taken over from the local2global package: ```TGraph```, ```NPGraph``` and ```JitGraph```. These include, among other things, methods for fast adjacency look-up and various optimizations. These are mostly used when performing graph clustering and generating patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.graphs import TGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = TGraph(cora[0].edge_index, edge_attr=cora[0].edge_attr, x=cora[0].x)\n",
    "print(tg.adj_index)\n",
    "print(tg.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a future iteration one can think about consolidating this part by having graphs represented in some existing graph package like Raphtory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter4'> üß© <font color=\"grey\">Patches </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A patch can equivalently refer to a subgraph or to an embedding of this subgraph. As a set of points, a patch is represented using the ```Patch``` class. A ```Patch``` object has the properties ```nodes```, ```index``` and ```coordinates```. ```nodes``` is simply a list of the nodes from the original graph that are present in the patch. ```index``` is a dict that maps each node to an index into ```coordinates```, which is just a list of coordinates. For example, if a graph embedding consists of four nodes in two dimensions as follows, and a patch is represented by the solid circles, then the corresponding object would have the following properties:\n",
    "\n",
    "![Patch](./images/square_patch.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.patch.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Patch([0,2,3], np.array([[0., 0.], [1., 0.], [1., 1.]]))\n",
    "print(p.coordinates)\n",
    "print(p.nodes)\n",
    "print(p.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.patch.clustering.fennel import fennel_clustering\n",
    "from l2gx.patch.clustering.metis import metis_clustering\n",
    "from l2gx.patch import generate_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches, patch_graph = generate_patches(tg, num_patches=10, clustering_method='metis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patches from the nodes of a **patch graph**, where two nodes are connected by an edge if the patches contain overlapping nodes. The alignment tasks consists of making the correponding coordinates overlap as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(p.nodes) for p in patches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches[0].coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter5'> üéØ <font color=\"grey\">Embedding </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2GX framework implements several graph embedding methods: ```SVDEmbedding```, ```GAEEmbedding```, ```VGAEEmbedding```, ```GraphSAGEEmbedding``` and ```DGIEmbedding```. The first three are based on transductive learning, while the last two are inductive.\n",
    "\n",
    "* <font color=\"grey\">SVD</font> - Classical spectral approach using eigendecomposition\n",
    "* <font color=\"grey\">GAE</font> - [Graph Auto-Encoder ](https://arxiv.org/abs/1611.07308) for deterministic reconstruction\n",
    "* <font color=\"grey\">VGAE</font> - [Variational Graph Auto-Encoder](https://arxiv.org/abs/1611.07308) with probabilistic latent variables\n",
    "* <font color=\"grey\">GraphSAGE</font> - [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) for scalable embedding\n",
    "* <font color=\"grey\">DGI</font> - [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) using self-supervised contrastive learning\n",
    "\n",
    "All methods are accessible through a unified interface with the ```get_embedding()``` function and registry system. The demonstration below shows convergence analysis, quality metrics, and UMAP visualizations for comprehensive comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.embedding import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Loading Cora dataset...\")\n",
    "cora_data = cora[0]\n",
    "print(f\"Cora dataset: {cora_data.num_nodes} nodes, {cora_data.num_edges} edges\")\n",
    "print(f\"Node features: {cora_data.x.shape}\")\n",
    "print(f\"Classes: {cora_data.y.unique().numel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_methods = ['svd', 'gae', 'vgae', 'dgi', 'graphsage']\n",
    "embeddings = {}\n",
    "embedding_times = {}\n",
    "training_histories = {}  # Store loss histories for each method\n",
    "\n",
    "# Function to capture and parse training output\n",
    "def capture_training_output(func, *args, **kwargs):\n",
    "    \"\"\"Capture stdout and parse training loss values.\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = captured_output = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        output = captured_output.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    # Parse loss values from output using regex\n",
    "    loss_pattern = r'Epoch\\s+(\\d+),\\s+Loss:\\s+([\\d.]+)'\n",
    "    matches = re.findall(loss_pattern, output)\n",
    "    \n",
    "    if matches:\n",
    "        epochs = [int(match[0]) for match in matches]\n",
    "        losses = [float(match[1]) for match in matches]\n",
    "        # Print the captured output so user can still see it\n",
    "        print(output, end='')\n",
    "        return result, (epochs, losses)\n",
    "    else:\n",
    "        print(output, end='')\n",
    "        return result, None\n",
    "\n",
    "for method in embedding_methods:\n",
    "    print(f\"üîÑ Computing {method} embedding...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if method == 'svd':\n",
    "            # SVD embedding (spectral approach) - no iterative training\n",
    "            embedder = get_embedding(method, embedding_dim=64)\n",
    "            embeddings[method] = embedder.fit_transform(cora_data)\n",
    "            training_histories[method] = None  # SVD doesn't have iterative training\n",
    "            \n",
    "        elif method == 'gae':\n",
    "            # Graph Auto-Encoder with loss tracking\n",
    "            embedder = get_embedding(method, embedding_dim=64, num_epochs=200)\n",
    "            result, loss_data = capture_training_output(embedder.fit_transform, cora_data)\n",
    "            embeddings[method] = result\n",
    "            if loss_data:\n",
    "                epochs, losses = loss_data\n",
    "                training_histories[method] = {'epochs': epochs, 'losses': losses}\n",
    "            else:\n",
    "                training_histories[method] = None\n",
    "                \n",
    "        elif method == 'vgae':\n",
    "            # Variational Graph Auto-Encoder with loss tracking\n",
    "            embedder = get_embedding(method, embedding_dim=64, num_epochs=200)\n",
    "            result, loss_data = capture_training_output(embedder.fit_transform, cora_data)\n",
    "            embeddings[method] = result\n",
    "            if loss_data:\n",
    "                epochs, losses = loss_data\n",
    "                training_histories[method] = {'epochs': epochs, 'losses': losses}\n",
    "            else:\n",
    "                training_histories[method] = None\n",
    "                \n",
    "        elif method == 'dgi':\n",
    "            # Deep Graph Infomax with loss tracking\n",
    "            embedder = get_embedding(method, embedding_dim=64, num_epochs=200)\n",
    "            result, loss_data = capture_training_output(embedder.fit_transform, cora_data)\n",
    "            embeddings[method] = result\n",
    "            if loss_data:\n",
    "                epochs, losses = loss_data\n",
    "                training_histories[method] = {'epochs': epochs, 'losses': losses}\n",
    "            else:\n",
    "                training_histories[method] = None\n",
    "                \n",
    "        elif method == 'graphsage':\n",
    "            # GraphSAGE inductive embedding with loss tracking\n",
    "            embedder = get_embedding(method, embedding_dim=64, num_epochs=200)\n",
    "            result, loss_data = capture_training_output(embedder.fit_transform, cora_data)\n",
    "            embeddings[method] = result\n",
    "            if loss_data:\n",
    "                epochs, losses = loss_data\n",
    "                training_histories[method] = {'epochs': epochs, 'losses': losses}\n",
    "            else:\n",
    "                training_histories[method] = None\n",
    "        \n",
    "        end_time = time.time()\n",
    "        embedding_times[method] = end_time - start_time\n",
    "        \n",
    "        print(f\"‚úÖ {method} embedding completed in {embedding_times[method]:.2f}s\")\n",
    "        print(f\"   Embedding shape: {embeddings[method].shape}\")\n",
    "        \n",
    "        # Print training info if available\n",
    "        if training_histories[method] is not None:\n",
    "            losses = training_histories[method]['losses']\n",
    "            epochs = training_histories[method]['epochs']\n",
    "            print(f\"   Captured {len(losses)} loss values from epochs {epochs[0]} to {epochs[-1]}\")\n",
    "            print(f\"   Initial loss: {losses[0]:.4f}, Final loss: {losses[-1]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {method} embedding failed: {str(e)}\")\n",
    "        # Create a placeholder embedding for visualization\n",
    "        embeddings[method] = torch.randn(cora_data.num_nodes, 64)\n",
    "        embedding_times[method] = 0.0\n",
    "        training_histories[method] = None\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"üìä Embedding computation summary:\")\n",
    "for method, emb in embeddings.items():\n",
    "    print(f\"   {method}: {emb.shape} ({embedding_times[method]:.2f}s)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Convergence\n",
    "print(\"üìà TRAINING CONVERGENCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count methods with training histories\n",
    "methods_with_history = [method for method in embedding_methods \n",
    "                       if training_histories[method] is not None]\n",
    "\n",
    "if methods_with_history:\n",
    "    # Create convergence plots with optimal layout\n",
    "    n_methods = len(methods_with_history)\n",
    "    \n",
    "    if n_methods <= 2:\n",
    "        # Single row for 1-2 methods\n",
    "        fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 5))\n",
    "        if n_methods == 1:\n",
    "            axes = [axes]\n",
    "    else:\n",
    "        # Two rows for 3+ methods\n",
    "        n_cols = min(3, n_methods)  # Max 3 columns\n",
    "        n_rows = (n_methods + n_cols - 1) // n_cols  # Ceiling division\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "        # Flatten axes array for easier indexing\n",
    "        if n_rows == 1:\n",
    "            axes = axes if isinstance(axes, list) else [axes]\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "    \n",
    "    fig.suptitle('Training Loss Convergence for Graph Embedding Methods', fontsize=16)\n",
    "    \n",
    "    # Color palette for different methods\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    for idx, method in enumerate(methods_with_history):\n",
    "        history_data = training_histories[method]\n",
    "        epochs = history_data['epochs']\n",
    "        losses = history_data['losses']\n",
    "        color = colors[idx % len(colors)]\n",
    "        \n",
    "        # Plot training loss\n",
    "        axes[idx].plot(epochs, losses, '-', color=color, linewidth=2, marker='o', \n",
    "                      markersize=4, label='Training Loss')\n",
    "        axes[idx].set_title(f'{method.upper()} Convergence')\n",
    "        axes[idx].set_xlabel('Epoch')\n",
    "        axes[idx].set_ylabel('Loss')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        axes[idx].legend()\n",
    "        \n",
    "        # Add convergence statistics\n",
    "        initial_loss = losses[0]\n",
    "        final_loss = losses[-1]\n",
    "        improvement = ((initial_loss - final_loss) / initial_loss) * 100\n",
    "        \n",
    "        # Add text box with statistics\n",
    "        textstr = f'Initial: {initial_loss:.4f}\\nFinal: {final_loss:.4f}\\nImprovement: {improvement:.1f}%\\nSamples: {len(losses)}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        axes[idx].text(0.05, 0.95, textstr, transform=axes[idx].transAxes, fontsize=9,\n",
    "                      verticalalignment='top', bbox=props)\n",
    "        \n",
    "        # Log scale option for better visualization if loss varies greatly\n",
    "        if max(losses) / min(losses) > 100:\n",
    "            axes[idx].set_yscale('log')\n",
    "            axes[idx].set_ylabel('Loss (log scale)')\n",
    "        \n",
    "        # Add trend line if we have enough points\n",
    "        if len(losses) > 2:\n",
    "            try:\n",
    "                from scipy.optimize import curve_fit\n",
    "                def exp_decay(x, a, b, c):\n",
    "                    return a * np.exp(-b * x) + c\n",
    "                \n",
    "                popt, _ = curve_fit(exp_decay, epochs, losses, maxfev=1000)\n",
    "                trend_epochs = np.linspace(min(epochs), max(epochs), 100)\n",
    "                trend_losses = exp_decay(trend_epochs, *popt)\n",
    "                axes[idx].plot(trend_epochs, trend_losses, '--', color=color, alpha=0.7, label='Trend')\n",
    "                axes[idx].legend()\n",
    "            except:\n",
    "                pass  # Skip trend line if fitting fails\n",
    "    \n",
    "    # Hide empty subplots if we have more axes than methods\n",
    "    if n_methods < len(axes):\n",
    "        for idx in range(n_methods, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics table\n",
    "    print(\"üìä Convergence Summary:\")\n",
    "    print(\"   Method      | Initial Loss | Final Loss | Improvement | Captured Points\")\n",
    "    print(\"   ------------|--------------|------------|-------------|----------------\")\n",
    "    for method in methods_with_history:\n",
    "        history_data = training_histories[method]\n",
    "        losses = history_data['losses']\n",
    "        epochs = history_data['epochs']\n",
    "        initial = losses[0]\n",
    "        final = losses[-1]\n",
    "        improvement = ((initial - final) / initial) * 100\n",
    "        n_points = len(losses)\n",
    "        print(f\"   {method:<11} | {initial:>10.4f}   | {final:>8.4f}   | {improvement:>8.1f}%    | {n_points:>13}\")\n",
    "    \n",
    "    # Additional convergence metrics\n",
    "    print()\n",
    "    print(\"üìä Detailed Convergence Metrics:\")\n",
    "    for method in methods_with_history:\n",
    "        history_data = training_histories[method]\n",
    "        losses = history_data['losses']\n",
    "        epochs = history_data['epochs']\n",
    "        \n",
    "        # Calculate convergence rate (loss reduction per epoch)\n",
    "        if len(losses) > 1:\n",
    "            total_epochs = epochs[-1] - epochs[0] + 1\n",
    "            loss_reduction = losses[0] - losses[-1]\n",
    "            convergence_rate = loss_reduction / total_epochs\n",
    "            \n",
    "            # Calculate stability (variance in final 25% of training)\n",
    "            final_quarter = losses[int(0.75 * len(losses)):]\n",
    "            stability = np.var(final_quarter)\n",
    "            \n",
    "            print(f\"   {method.upper()}:\")\n",
    "            print(f\"     ‚Ä¢ Convergence rate: {convergence_rate:.6f} loss/epoch\")\n",
    "            print(f\"     ‚Ä¢ Final quarter stability (variance): {stability:.6f}\")\n",
    "            print(f\"     ‚Ä¢ Training span: epochs {epochs[0]} to {epochs[-1]}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No iterative training methods detected or loss tracking failed.\")\n",
    "    print(\"   SVD is a direct spectral method without iterative optimization.\")\n",
    "\n",
    "# Always show computation time comparison\n",
    "print()\n",
    "print(\"‚è±Ô∏è  COMPUTATION TIME COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "methods_list = list(embedding_methods)\n",
    "times_list = [embedding_times[method] for method in methods_list]\n",
    "\n",
    "# Create color-coded bars\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'plum']\n",
    "bars = plt.bar(methods_list, times_list, alpha=0.7, color=colors[:len(methods_list)])\n",
    "plt.title('Embedding Method Computation Times Comparison')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add time labels on bars\n",
    "for bar, time_val in zip(bars, times_list):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "            f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare node labels for quality evaluation\n",
    "node_labels = cora_data.y.numpy()\n",
    "\n",
    "silhouette_scores = {}\n",
    "for method in embedding_methods:\n",
    "    if torch.is_tensor(embeddings[method]):\n",
    "        embedding_np = embeddings[method].detach().numpy()\n",
    "    else:\n",
    "        embedding_np = embeddings[method]\n",
    "    \n",
    "    # Calculate silhouette score (higher is better)\n",
    "    silhouette_scores[method] = silhouette_score(embedding_np, node_labels)\n",
    "\n",
    "print(\"üìä Embedding Quality Metrics:\")\n",
    "print(\"   Method          | Silhouette Score | Computation Time\")\n",
    "print(\"   ----------------|------------------|------------------\")\n",
    "for method in embedding_methods:\n",
    "    score = silhouette_scores[method]\n",
    "    time_taken = embedding_times[method]\n",
    "    print(f\"   {method:<15} | {score:>13.3f}    | {time_taken:>13.2f}s\")\n",
    "\n",
    "print()\n",
    "print(\"üìà Interpretation:\")\n",
    "print(\"   ‚Ä¢ Silhouette Score: Measures how well-separated the clusters are\")\n",
    "print(\"   ‚Ä¢ Higher scores indicate better embedding quality for classification\")\n",
    "print(\"   ‚Ä¢ Consider both quality and computation time for practical applications\")\n",
    "print()\n",
    "\n",
    "# Highlight the best performing method\n",
    "best_method = max(silhouette_scores.keys(), key=lambda k: silhouette_scores[k])\n",
    "print(f\"üèÜ Best performing method: {best_method} (Silhouette Score: {silhouette_scores[best_method]:.3f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üó∫Ô∏è  UMAP VISUALIZATION OF EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare node labels for coloring\n",
    "node_labels = cora_data.y.numpy()\n",
    "class_names = ['Case_Based', 'Genetic_Algorithms', 'Neural_Networks', \n",
    "               'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory']\n",
    "\n",
    "# Create UMAP visualizations with optimal layout for 5 methods\n",
    "n_methods = len(embedding_methods)\n",
    "n_cols = 3  # 3 columns\n",
    "n_rows = (n_methods + n_cols - 1) // n_cols  # Ceiling division: 2 rows for 5 methods\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "axes = axes.flatten()  # Flatten for easier indexing\n",
    "\n",
    "fig.suptitle('UMAP Visualization of Graph Embeddings on Cora Dataset', fontsize=18)\n",
    "\n",
    "# Color palette for different methods\n",
    "method_colors = ['navy', 'darkred', 'darkgreen', 'darkorange', 'purple']\n",
    "\n",
    "for idx, method in enumerate(embedding_methods):\n",
    "    print(f\"üîÑ Creating UMAP visualization for {method}...\")\n",
    "    \n",
    "    # Convert embedding to numpy if it's a tensor\n",
    "    if torch.is_tensor(embeddings[method]):\n",
    "        embedding_np = embeddings[method].detach().numpy().astype(np.float32)\n",
    "    else:\n",
    "        embedding_np = np.array(embeddings[method], dtype=np.float32)\n",
    "    \n",
    "    # Ensure data is finite and normalized\n",
    "    embedding_np = np.nan_to_num(embedding_np, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    \n",
    "    # Apply UMAP for dimensionality reduction with robust parameters\n",
    "    umap_model = UMAP(\n",
    "        n_components=2, \n",
    "        random_state=42, \n",
    "        n_neighbors=min(15, len(embedding_np) - 1),  # Ensure n_neighbors < n_samples\n",
    "        min_dist=0.1,\n",
    "        metric='euclidean',\n",
    "        n_jobs=1,  # Avoid threading warnings\n",
    "        low_memory=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    embedding_2d = umap_model.fit_transform(embedding_np)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    scatter = axes[idx].scatter(embedding_2d[:, 0], embedding_2d[:, 1], \n",
    "                               c=node_labels, cmap='tab10', alpha=0.6, s=15)\n",
    "    \n",
    "    # Add method-specific styling\n",
    "    axes[idx].set_title(f'{method.upper()} Embedding\\n(Time: {embedding_times[method]:.2f}s)', \n",
    "                       fontsize=12, color=method_colors[idx % len(method_colors)])\n",
    "    axes[idx].set_xlabel('UMAP Component 1')\n",
    "    axes[idx].set_ylabel('UMAP Component 2')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add border color to distinguish methods\n",
    "    for spine in axes[idx].spines.values():\n",
    "        spine.set_edgecolor(method_colors[idx % len(method_colors)])\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(n_methods, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "# Add a single colorbar for all plots\n",
    "cbar = fig.colorbar(scatter, ax=axes[:n_methods], orientation='horizontal', \n",
    "                   pad=0.05, aspect=40, shrink=0.8)\n",
    "cbar.set_label('Node Classes (Cora Paper Categories)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Re-enable warnings for subsequent code\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter6'> üîó <font color=\"grey\">Alignment </font></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.align import get_aligner\n",
    "from examples.example import (\n",
    "    generate_points, \n",
    "    voronoi_patches, \n",
    "    plot_patches, \n",
    "    transform_patches\n",
    ")\n",
    "from l2gx.align import procrustes_error\n",
    "rg = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = generate_points(n_clusters=5, scale=1.0, std=0.2, max_size=2000, min_size=128, dim=2)\n",
    "patches, centers = voronoi_patches(points, sample_size=10, min_degree=4, min_overlap=64, min_patch_size=128, eps=1, kmeans=False)\n",
    "overlap_count = 0\n",
    "for i in range(len(patches)):\n",
    "    for j in range(i+1, len(patches)):\n",
    "        overlap = len(set(patches[i].nodes) & set(patches[j].nodes))\n",
    "        if overlap >= 64:\n",
    "            overlap_count += 1\n",
    "\n",
    "print(f\"Patch pairs with sufficient overlap (‚â•64): {overlap_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0 # 0.1\n",
    "shift_scale = 10\n",
    "scale_range = None #[0.01, 100]\n",
    "\n",
    "# Create transformed copies of the patches\n",
    "transformed_patches = [copy.deepcopy(p) for p in patches]\n",
    "\n",
    "# Add noise to the transformed patches\n",
    "if noise_level > 0:\n",
    "    for patch in transformed_patches:\n",
    "        noise = rg.normal(loc=0, scale=noise_level, size=patch.coordinates.shape)\n",
    "        patch.coordinates += noise\n",
    "\n",
    "transformed_patches = transform_patches(\n",
    "    transformed_patches, \n",
    "    shift_scale=shift_scale, \n",
    "    scale_range=scale_range\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(patches, transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = get_aligner(\"geo2\", verbose=True, min_overlap=64)\n",
    "result = aligner.align_patches(transformed_patches, \n",
    "                      num_epochs=10000,\n",
    "                      learning_rate=0.01,\n",
    "                      use_orthogonal_reg=True,\n",
    "                      orthogonal_reg_weight=1000,\n",
    "                      use_bfs_training=True,\n",
    "                      center_patches=False,\n",
    "                      device=\"cpu\" \n",
    "                      )\n",
    "print(f\"Alignment completed. Final loss: {result.loss_history[-1]:.6f}\")\n",
    "aligned_embedding = result.get_aligned_embedding()\n",
    "\n",
    "_, _, procrustes_error = procrustes(points, aligned_embedding)\n",
    "print(f\"Procrustes error: {procrustes_error:.6f}\")\n",
    "\n",
    "plot_patches(patches, result.patches)\n",
    "\n",
    "intermediate_patches = copy.deepcopy(result.patches)\n",
    "result = aligner.align_patches(intermediate_patches, \n",
    "                      num_epochs=10000,\n",
    "                      learning_rate=0.01,\n",
    "                      use_orthogonal_reg=True,\n",
    "                      orthogonal_reg_weight=1000,\n",
    "                      use_bfs_training=True,\n",
    "                      center_patches=False,\n",
    "                      device=\"cpu\" \n",
    "                      )\n",
    "\n",
    "_, _, procrustes_error = procrustes(points, aligned_embedding)\n",
    "print(f\"Procrustes error: {procrustes_error:.6f}\")\n",
    "\n",
    "plot_patches(patches, result.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(patches, aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = generate_points(n_clusters=5, scale=1.0, std=0.2, max_size=2000, min_size=128, dim=2)\n",
    "patches, centers = voronoi_patches(points, sample_size=2, min_degree=1, min_overlap=64, min_patch_size=128, eps=1, kmeans=False)\n",
    "overlap_count = 0\n",
    "for i in range(len(patches)):\n",
    "    for j in range(i+1, len(patches)):\n",
    "        overlap = len(set(patches[i].nodes) & set(patches[j].nodes))\n",
    "        if overlap >= 64:\n",
    "            overlap_count += 1\n",
    "\n",
    "print(f\"Patch pairs with sufficient overlap (‚â•64): {overlap_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0 # 0.1\n",
    "shift_scale = 10\n",
    "scale_range = None #[0.01, 100]\n",
    "\n",
    "# Create transformed copies of the patches\n",
    "transformed_patches = [copy.deepcopy(p) for p in patches]\n",
    "\n",
    "# Add noise to the transformed patches\n",
    "if noise_level > 0:\n",
    "    for patch in transformed_patches:\n",
    "        noise = rg.normal(loc=0, scale=noise_level, size=patch.coordinates.shape)\n",
    "        patch.coordinates += noise\n",
    "\n",
    "transformed_patches = transform_patches(\n",
    "    transformed_patches, \n",
    "    shift_scale=shift_scale, \n",
    "    scale_range=scale_range\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(patches, transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = get_aligner(\"geo2\", verbose=True, min_overlap=64)\n",
    "result = aligner.align_patches(transformed_patches, \n",
    "                      num_epochs=1000,\n",
    "                      learning_rate=0.01,\n",
    "                      use_orthogonal_reg=True,\n",
    "                      orthogonal_reg_weight=10,\n",
    "                      device=\"cpu\",\n",
    "                      center_patches=False\n",
    "                      )\n",
    "print(f\"Alignment completed. Final loss: {result.loss_history[-1]:.6f}\")\n",
    "aligned_embedding = result.get_aligned_embedding()\n",
    "\n",
    "_, _, procrustes_error = procrustes(points, aligned_embedding)\n",
    "print(f\"Procrustes error: {procrustes_error:.6f}\")\n",
    "\n",
    "plot_patches(patches, aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner = get_aligner(\n",
    "    \"geo\",\n",
    "    num_epochs=10000,\n",
    "    learning_rate=0.001,\n",
    "    model_type=\"affine\",\n",
    "    use_orthogonal_reg=True,\n",
    "    orthogonal_reg_weight=10,\n",
    "    center_patches=True,\n",
    "    preserve_scale=False,\n",
    "    min_overlap=64,\n",
    "    verbose=True)\n",
    "geo_aligner.align_patches(transformed_patches, min_overlap=64, scale=False)\n",
    "embedding = geo_aligner.get_aligned_embedding()\n",
    "plot_patches(patches, geo_aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procrustes_error(points, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner.debug_alignment_step_by_step(transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner.diagnose_alignment_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner.check_overlap_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(geo_aligner.loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2g_aligner = get_aligner(\"l2g\")\n",
    "l2g_aligner.align_patches(transformed_patches, scale=True)\n",
    "embedding = l2g_aligner.get_aligned_embedding()\n",
    "plot_patches(patches, l2g_aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procrustes_error(points, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter7'> <font color=\"grey\">7. Hierarchical alignment </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter8'> <font color=\"grey\">8. Visualisation </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualisation, it is convenient to use external packages such as Heimdall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
