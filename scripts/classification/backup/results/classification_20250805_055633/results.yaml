classification_results:
  test:
    accuracy: 0.7416974169741697
    classification_report:
      '0':
        f1-score: 0.5882352941176471
        precision: 0.6060606060606061
        recall: 0.5714285714285714
        support: 70.0
      '1':
        f1-score: 0.7272727272727273
        precision: 0.7111111111111111
        recall: 0.7441860465116279
        support: 43.0
      '2':
        f1-score: 0.8235294117647058
        precision: 0.813953488372093
        recall: 0.8333333333333334
        support: 84.0
      '3':
        f1-score: 0.8072289156626506
        precision: 0.7976190476190477
        recall: 0.8170731707317073
        support: 164.0
      '4':
        f1-score: 0.7544910179640718
        precision: 0.7682926829268293
        recall: 0.7411764705882353
        support: 85.0
      '5':
        f1-score: 0.6495726495726496
        precision: 0.6666666666666666
        recall: 0.6333333333333333
        support: 60.0
      '6':
        f1-score: 0.6756756756756757
        precision: 0.6578947368421053
        recall: 0.6944444444444444
        support: 36.0
      accuracy: 0.7416974169741697
      macro avg:
        f1-score: 0.7180008131471611
        precision: 0.7173711913712085
        recall: 0.7192821957673219
        support: 542.0
      weighted avg:
        f1-score: 0.7406672148404095
        precision: 0.740171080461063
        recall: 0.7416974169741697
        support: 542.0
    confusion_matrix:
    - - 40
      - 5
      - 1
      - 11
      - 4
      - 7
      - 2
    - - 0
      - 32
      - 4
      - 3
      - 1
      - 1
      - 2
    - - 1
      - 2
      - 70
      - 5
      - 2
      - 3
      - 1
    - - 11
      - 4
      - 4
      - 134
      - 8
      - 2
      - 1
    - - 7
      - 0
      - 1
      - 12
      - 63
      - 1
      - 1
    - - 5
      - 1
      - 3
      - 3
      - 4
      - 38
      - 6
    - - 2
      - 1
      - 3
      - 0
      - 0
      - 5
      - 25
    f1_macro: 0.7180008131471611
    f1_micro: 0.7416974169741697
    f1_weighted: 0.7406672148404095
  train:
    accuracy: 0.8744063324538258
    classification_report:
      '0':
        f1-score: 0.7820773930753564
        precision: 0.7836734693877551
        recall: 0.7804878048780488
        support: 246.0
      '1':
        f1-score: 0.9174917491749175
        precision: 0.9205298013245033
        recall: 0.9144736842105263
        support: 152.0
      '2':
        f1-score: 0.9601386481802426
        precision: 0.9719298245614035
        recall: 0.9486301369863014
        support: 292.0
      '3':
        f1-score: 0.8794567062818336
        precision: 0.8547854785478548
        recall: 0.9055944055944056
        support: 572.0
      '4':
        f1-score: 0.8502581755593803
        precision: 0.875886524822695
        recall: 0.8260869565217391
        support: 299.0
      '5':
        f1-score: 0.8571428571428571
        precision: 0.8634146341463415
        recall: 0.8509615384615384
        support: 208.0
      '6':
        f1-score: 0.8663967611336032
        precision: 0.8842975206611571
        recall: 0.8492063492063492
        support: 126.0
      accuracy: 0.8744063324538258
      macro avg:
        f1-score: 0.8732803272211701
        precision: 0.8792167504931013
        recall: 0.8679201251227012
        support: 1895.0
      weighted avg:
        f1-score: 0.8743738292948643
        precision: 0.8751170491819239
        recall: 0.8744063324538258
        support: 1895.0
    confusion_matrix:
    - - 192
      - 2
      - 3
      - 27
      - 8
      - 8
      - 6
    - - 4
      - 139
      - 3
      - 4
      - 0
      - 2
      - 0
    - - 3
      - 3
      - 277
      - 6
      - 1
      - 1
      - 1
    - - 14
      - 4
      - 2
      - 518
      - 22
      - 9
      - 3
    - - 11
      - 1
      - 0
      - 35
      - 247
      - 4
      - 1
    - - 12
      - 2
      - 0
      - 13
      - 1
      - 177
      - 3
    - - 9
      - 0
      - 0
      - 3
      - 3
      - 4
      - 107
    f1_macro: 0.8732803272211701
    f1_micro: 0.8744063324538258
    f1_weighted: 0.8743738292948643
  val:
    accuracy: 0.7195571955719557
    classification_report:
      '0':
        f1-score: 0.647887323943662
        precision: 0.6388888888888888
        recall: 0.6571428571428571
        support: 35.0
      '1':
        f1-score: 0.7659574468085106
        precision: 0.72
        recall: 0.8181818181818182
        support: 22.0
      '2':
        f1-score: 0.7692307692307693
        precision: 0.7142857142857143
        recall: 0.8333333333333334
        support: 42.0
      '3':
        f1-score: 0.7368421052631579
        precision: 0.8
        recall: 0.6829268292682927
        support: 82.0
      '4':
        f1-score: 0.7391304347826086
        precision: 0.68
        recall: 0.8095238095238095
        support: 42.0
      '5':
        f1-score: 0.7213114754098361
        precision: 0.7096774193548387
        recall: 0.7333333333333333
        support: 30.0
      '6':
        f1-score: 0.5
        precision: 0.7
        recall: 0.3888888888888889
        support: 18.0
      accuracy: 0.7195571955719557
      macro avg:
        f1-score: 0.6971942222055063
        precision: 0.7089788603613488
        recall: 0.703332981381762
        support: 271.0
      weighted avg:
        f1-score: 0.715640913764765
        precision: 0.7241750320729015
        recall: 0.7195571955719557
        support: 271.0
    confusion_matrix:
    - - 23
      - 2
      - 4
      - 1
      - 3
      - 1
      - 1
    - - 0
      - 18
      - 2
      - 1
      - 0
      - 1
      - 0
    - - 0
      - 1
      - 35
      - 4
      - 0
      - 1
      - 1
    - - 4
      - 4
      - 5
      - 56
      - 11
      - 1
      - 1
    - - 3
      - 0
      - 0
      - 5
      - 34
      - 0
      - 0
    - - 2
      - 0
      - 3
      - 1
      - 2
      - 22
      - 0
    - - 4
      - 0
      - 0
      - 2
      - 0
      - 5
      - 7
    f1_macro: 0.6971942222055063
    f1_micro: 0.7195571955719557
    f1_weighted: 0.715640913764765
configuration:
  classification_config:
    classifier:
      params:
        max_iter: 1000
        multi_class: auto
        random_state: 42
        solver: lbfgs
      type: logistic_regression
    evaluation:
      cross_validation:
        enabled: false
        folds: 5
      random_state: 42
      stratify: true
      test_size: 0.2
      val_size: 0.1
    preprocessing:
      handle_imbalanced: false
      scale_features: true
  embedding_config:
    alignment:
      method: l2g
      randomized_method: randomized
      scale: true
      sketch_method: rademacher
      verbose: true
    dataset:
      name: Cora
      normalize_features: false
    embedding:
      embedding_dim: 128
      epochs: 10000
      hidden_dim_multiplier: 2
      learning_rate: 0.001
      method: vgae
      patience: 20
      verbose: false
    experiment:
      description: L2G alignment with VGAE base embeddings and randomized Rademacher
        synchronization
      name: L2G_VGAE_Rademacher_Experiment
      output_dir: results/l2g_experiment
    output:
      format: npz
      save_alignment_matrices: false
      save_embeddings: true
      save_patches: false
    patches:
      clustering_method: metis
      hierarchical:
        enabled: false
        k_patches: 3
        max_patch_size: 500
        min_patch_size: 50
      min_overlap: 256
      num_patches: 10
      sparsify_method: resistance
      target_overlap: 512
      target_patch_degree: 4
      use_conductance_weighting: true
      verbose: false
embedding_metadata:
  config:
    alignment:
      method: l2g
      randomized_method: randomized
      scale: true
      sketch_method: rademacher
      verbose: true
    dataset:
      name: Cora
      normalize_features: false
    embedding:
      embedding_dim: 128
      epochs: 10000
      hidden_dim_multiplier: 2
      learning_rate: 0.001
      method: vgae
      patience: 20
      verbose: false
    experiment:
      description: L2G alignment with VGAE base embeddings and randomized Rademacher
        synchronization
      name: L2G_VGAE_Rademacher_Experiment
      output_dir: results/l2g_experiment
    output:
      format: npz
      save_alignment_matrices: false
      save_embeddings: true
      save_patches: false
    patches:
      clustering_method: metis
      hierarchical:
        enabled: false
        k_patches: 3
        max_patch_size: 500
        min_patch_size: 50
      min_overlap: 256
      num_patches: 10
      sparsify_method: resistance
      target_overlap: 512
      target_patch_degree: 4
      use_conductance_weighting: true
      verbose: false
  dataset:
    name: Cora
    num_classes: 7
    num_edges: 10556
    num_features: 1433
    num_nodes: 2708
  embedding_shape: !!python/tuple
  - 2708
  - 128
  embedding_type: patched
  num_patches: 10
timestamp: '2025-08-05T05:56:46.073085'
