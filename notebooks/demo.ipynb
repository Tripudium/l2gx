{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.spatial import procrustes\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from l2gx.align.utils import to_device\n",
    "import networkx as nx\n",
    "from scipy.stats import special_ortho_group\n",
    "from umap import UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "# Suppress common UMAP and sklearn warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, SpectralClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåå <font color=\"grey\"> Local2Global X - Graph Representation Learning at Scale</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"grey\">  Table of Contents</font>\n",
    "\n",
    "üèóÔ∏è <a href='#chapter1'>Structure</a>\n",
    "\n",
    "üìä <a href='#chapter2'>Datasets</a>\n",
    "\n",
    "üåê <a href='#chapter3'>Graphs</a>\n",
    "\n",
    "üß© <a href='#chapter4'>Patches</a>\n",
    "\n",
    "üéØ <a href='#chapter5'>Embedding</a>\n",
    "\n",
    "üîó <a href='#chapter6'>Alignment</a>\n",
    "\n",
    "üå≥ <a href='#chapter7'>Hierarchical alignment</a>\n",
    "\n",
    "üìà <a href='#chapter8'>Visualisation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter1'> üèóÔ∏è <font color=\"grey\">Structure </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five main parts to the package, organised as follows.\n",
    "\n",
    "```\n",
    "l2gv2/\n",
    "‚îú‚îÄ‚îÄ datasets/\n",
    "‚îú‚îÄ‚îÄ graphs/\n",
    "‚îú‚îÄ‚îÄ patch/\n",
    "‚îú‚îÄ‚îÄ embedding/\n",
    "‚îî‚îÄ‚îÄ align/\n",
    "    ‚îú‚îÄ‚îÄ l2g/\n",
    "    ‚îî‚îÄ‚îÄ geo/\n",
    "```\n",
    "\n",
    "A brief overview of the contents:\n",
    "\n",
    "* ```datasets``` contains interfaces are provided for various common benchmark datasets. \n",
    "* ```graphs``` contains wrappers for graphs represented as lists of edges in pytorch-geometric ```data.edge_index``` format. These implemented features such as fast adjacency look-up and a variety of algorithms on graphs.\n",
    "* ```patch``` directory contains datastructures to represent patches and patch graphs, as well as methods to subdivide a graph into patches. \n",
    "* ```embedding``` contains various graph embedding methods, including Graph Autoencoders (GAE) and [Variational Graph Autoencoders (VGAE)](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.VGAE.html).\n",
    "* ```align``` contains two methods to compute the alignment of patches into a single graph embedding: eigenvalue synchronisation based on the [Local2Global](https://link.springer.com/article/10.1007/s10994-022-06285-7) algorithm, and the new method based on learning the alignment using a one-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter2'> üìä <font color=\"grey\">Datasets </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2GX framework provides access to multiple graph datasets spanning different domains and scales. All datasets are accessible through the unified `get_dataset()` interface and support conversion between multiple formats (PyTorch Geometric, Raphtory, Polars).\n",
    "\n",
    "| Dataset | Type | Nodes | Edges | Features | Domain |\n",
    "|---------|------|-------|-------|----------|--------|\n",
    "| **Cora** | Static Citation | 2,708 | 10,556 | 1,433 | üìö Academic Papers |\n",
    "| **CiteSeer** | Static Citation | 3,327 | 9,104 | 3,703 | üìö Academic Papers |\n",
    "| **PubMed** | Static Citation | 19,717 | 88,648 | 500 | üìö Academic Papers |\n",
    "| **AS-733** | Temporal Network | 7,716 | 45,645 | Temporal | üåê Internet Infrastructure |\n",
    "| **DGraph** | Financial | ~3M | ~4M | Multiple | üí∞ Fraud Detection |\n",
    "| **Elliptic** | Bitcoin | 203,769 | 234,355 | 166 | ‚Çø Cryptocurrency |\n",
    "| **MAG240M** | Academic | 244M+ | 1.7B+ | Rich | üéì Citation Graph |\n",
    "| **ORBITAAL** | Bitcoin Temporal | 252M (1K sample) | 785M (5K sample) | Temporal + Anomaly | ‚Çø Financial Fraud |\n",
    "\n",
    "#### Dataset Details\n",
    "\n",
    "* **Cora**: The [Cora dataset](https://graphsandnetworks.com/the-cora-dataset/) is a citation network of 2,708 scientific publications divided into 7 classes. Each node has a 1,433-dimensional feature vector indicating word presence/absence. Accessed through PyTorch Geometric's [Planetoid](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset.\n",
    "\n",
    "* **CiteSeer**: The [CiteSeer dataset](https://citeseerx.ist.psu.edu/) is a citation network of 3,327 scientific publications classified into 6 classes (Agents, AI, DB, IR, ML, HCI). Each node has a 3,703-dimensional feature vector. Part of the Planetoid collection, this dataset provides a slightly larger and more challenging benchmark than Cora.\n",
    "\n",
    "* **PubMed**: The [PubMed dataset](https://pubmed.ncbi.nlm.nih.gov/) is the largest Planetoid dataset with 19,717 scientific publications from the PubMed database classified into 3 diabetes-related classes. Each node has a 500-dimensional TF-IDF feature vector derived from paper abstracts. Excellent for testing scalability of graph neural network methods.\n",
    "\n",
    "* **AS-733**: The [SNAP autonomous systems AS-733](https://snap.stanford.edu/data/as-733.html) dataset contains 733 daily snapshots spanning 785 days (November 1997 to January 2000). Nodes represent autonomous systems and edges indicate communication events.\n",
    "\n",
    "* **DGraph**: [DGraph](https://dgraph.xinye.com/dataset) is a real-world financial graph for anomaly detection research. Described in [DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection](https://arxiv.org/abs/2207.03579). Requires manual download.\n",
    "\n",
    "* **Elliptic**: The [Elliptic dataset](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set) maps Bitcoin transactions to licit/illicit categories. Contains 203,769 transactions with 166 features each. Used in [Anti-Money Laundering in Bitcoin](https://arxiv.org/pdf/1908.02591) research. Requires manual download from Kaggle.\n",
    "\n",
    "* **MAG240M**: The [MAG240M](https://ogb.stanford.edu/docs/lsc/mag240m/) dataset is a large heterogeneous academic citation graph with 244+ million nodes (papers, authors, institutions, fields) and 1.7+ billion edges. Requires the OGB library and substantial storage (~100GB).\n",
    "\n",
    "* **ORBITAAL**: The [ORBITAAL](https://www.nature.com/articles/s41597-025-04595-8) dataset is a comprehensive temporal Bitcoin transaction graph covering 13 years (2009-2021) with 252M entities and 785M transactions. Features timestamped transactions, entity types (exchanges, wallets, services, miners), and anomaly labels for financial fraud detection. Ideal for temporal graph neural networks and cryptocurrency flow analysis.\n",
    "\n",
    "\n",
    "For the datasets requiring manual download, provide the path:\n",
    "```\n",
    "elliptic = get_dataset(\"Elliptic\", source_file=\"/path/to/elliptic.zip\")\n",
    "dgraph = get_dataset(\"DGraph\", source_file=\"/path/to/dgraph.zip\")\n",
    "```\n",
    "\n",
    "All datasets support conversion to different formats and follow the PyTorch Geometric convention. Temporal graphs return iterables over time slices, and graphs can be exported to Raphtory or NetworkX formats for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current datasets: ['as-733', 'Cora', 'DGraph', 'Elliptic', 'MAG240M', 'ORBITAAL']\n"
     ]
    }
   ],
   "source": [
    "from l2gx.datasets import get_dataset, list_available_datasets\n",
    "datasets = list_available_datasets()\n",
    "print(f\"Current datasets: {datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "cora = get_dataset(\"Cora\")\n",
    "tg_cora = cora.to(\"torch-geometric\")\n",
    "print(tg_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edge and node data from memory\n",
      "Graph: 2708 nodes, 5278 edges\n",
      "Labels: 7 unique classes\n"
     ]
    }
   ],
   "source": [
    "G = cora.to(\"networkx\")\n",
    "labels = tg_cora.y.numpy()\n",
    "print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"Labels: {len(np.unique(labels))} unique classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter3'> üåê <font color=\"grey\">Graphs </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```TGraph``` is a wrapper for torch-geometric ```Data`` objects. These include, among other things, methods for fast adjacency look-up and various optimizations. These are mostly used when performing graph clustering and generating patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.graphs import TGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,     3,     6,  ..., 10548, 10552, 10556])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tg = TGraph(cora[0].edge_index, edge_attr=cora[0].edge_attr, x=cora[0].x)\n",
    "print(tg.adj_index)\n",
    "print(tg.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a future iteration one can think about consolidating this part by having graphs represented in some existing graph package like Raphtory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter4'> üß© <font color=\"grey\">Patches </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A patch can equivalently refer to a subgraph or to an embedding of this subgraph. As a set of points, a patch is represented using the ```Patch``` class. A ```Patch``` object has the properties ```nodes```, ```index``` and ```coordinates```. ```nodes``` is simply a list of the nodes from the original graph that are present in the patch. ```index``` is a dict that maps each node to an index into ```coordinates```, which is just a list of coordinates. For example, if a graph embedding consists of four nodes in two dimensions as follows, and a patch is represented by the solid circles, then the corresponding object would have the following properties:\n",
    "\n",
    "![Patch](./images/square_patch.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.patch.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "[0 2 3]\n",
      "{0: 0, 2: 1, 3: 2}\n"
     ]
    }
   ],
   "source": [
    "p = Patch([0,2,3], np.array([[0., 0.], [1., 0.], [1., 1.]]))\n",
    "print(p.coordinates)\n",
    "print(p.nodes)\n",
    "print(p.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.patch.clustering.fennel import fennel_clustering\n",
    "from l2gx.patch.clustering.metis import metis_clustering\n",
    "from l2gx.patch import generate_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating patches from graph with 2708 nodes, 10556 edges\n",
      "Target patches: 10 ‚Üí ~270 nodes per patch\n",
      "Overlap: min=27, target=54\n",
      "Step 1: Clustering with metis...\n",
      "Clustering complete: 10 clusters, sizes: [260, 284]\n",
      "Step 2: Creating patches with resistance sparsification...\n",
      "number of patches: 10\n",
      "average patch degree: 3.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34f9955d59a4a8abeece53914ccf1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "enlarging patch overlaps:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch generation complete: 10 patches created\n",
      "Patch sizes: [305, 411], avg: 346.8\n"
     ]
    }
   ],
   "source": [
    "graph = TGraph(tg_cora.edge_index)\n",
    "patches, patch_graph = generate_patches(graph, num_patches=10, clustering_method='metis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patches from the nodes of a **patch graph**, where two nodes are connected by an edge if the patches contain overlapping nodes. The alignment tasks consists of making the correponding coordinates overlap as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[360, 305, 324, 411, 337, 346, 352, 331, 358, 344]\n"
     ]
    }
   ],
   "source": [
    "print([len(p.nodes) for p in patches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.embedding import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<l2gx.graphs.tgraph.TGraph at 0x323f0b3d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.subgraph(patches[0].nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 6.7392\n",
      "Epoch 050, Loss: 1.7403\n",
      "Epoch 100, Loss: 1.8981\n",
      "Epoch 150, Loss: 1.6950\n"
     ]
    }
   ],
   "source": [
    "embedding = get_embedding('vgae', embedding_dim=64, num_epochs=200)\n",
    "coords = embedding.fit_transform(graph.subgraph(patches[0].nodes).to_tg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000, Loss: 9.4831\n",
      "Epoch 050, Loss: 2.1421\n",
      "Epoch 100, Loss: 1.9707\n",
      "Epoch 150, Loss: 2.0556\n",
      "Epoch 000, Loss: 10.5114\n",
      "Epoch 050, Loss: 2.2381\n",
      "Epoch 100, Loss: 2.3584\n",
      "Epoch 150, Loss: 1.8994\n",
      "Epoch 000, Loss: 9.9179\n",
      "Epoch 050, Loss: 1.9891\n",
      "Epoch 100, Loss: 2.1991\n",
      "Epoch 150, Loss: 1.8927\n",
      "Epoch 000, Loss: 10.5973\n",
      "Epoch 050, Loss: 2.8249\n",
      "Epoch 100, Loss: 1.7679\n",
      "Epoch 150, Loss: 2.0111\n",
      "Epoch 000, Loss: 10.1020\n",
      "Epoch 050, Loss: 2.1600\n",
      "Epoch 100, Loss: 1.9939\n",
      "Epoch 150, Loss: 2.0981\n",
      "Epoch 000, Loss: 9.9532\n",
      "Epoch 050, Loss: 1.8637\n",
      "Epoch 100, Loss: 1.8521\n",
      "Epoch 150, Loss: 1.7260\n",
      "Epoch 000, Loss: 11.5251\n",
      "Epoch 050, Loss: 2.0597\n",
      "Epoch 100, Loss: 1.9989\n",
      "Epoch 150, Loss: 2.2109\n",
      "Epoch 000, Loss: 9.7543\n",
      "Epoch 050, Loss: 1.9767\n",
      "Epoch 100, Loss: 1.8462\n",
      "Epoch 150, Loss: 1.7611\n",
      "Epoch 000, Loss: 10.2512\n",
      "Epoch 050, Loss: 2.0530\n",
      "Epoch 100, Loss: 1.7046\n",
      "Epoch 150, Loss: 1.9051\n",
      "Epoch 000, Loss: 9.3731\n",
      "Epoch 050, Loss: 2.0387\n",
      "Epoch 100, Loss: 2.0723\n",
      "Epoch 150, Loss: 2.1459\n",
      "VGAE embedding completed in 6.97s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for patch in patches:\n",
    "    embedder = get_embedding('vgae', embedding_dim=128, num_epochs=500)\n",
    "    patch.coordinates = embedder.fit_transform(graph.subgraph(patch.nodes).to_tg())\n",
    "\n",
    "end_time = time.time()\n",
    "embedding_time = end_time - start_time\n",
    "\n",
    "print(f\"VGAE embedding completed in {embedding_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.align import get_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArpackError",
     "evalue": "ARPACK error -9: Starting vector is zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArpackError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m l2g_aligner \u001b[38;5;241m=\u001b[39m get_aligner(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2g\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43ml2g_aligner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malign_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m embedding \u001b[38;5;241m=\u001b[39m l2g_aligner\u001b[38;5;241m.\u001b[39mget_aligned_embedding()\n",
      "File \u001b[0;32m~/Projects/G2007/code/L2GX/l2gx/align/l2g/local2global.py:171\u001b[0m, in \u001b[0;36mL2GAlignmentProblem.align_patches\u001b[0;34m(self, patches, min_overlap, scale)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_patches(patches, min_overlap)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate_patches()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_patches()\n",
      "File \u001b[0;32m~/Projects/G2007/code/L2GX/l2gx/align/alignment.py:160\u001b[0m, in \u001b[0;36mAlignmentProblem.scale_patches\u001b[0;34m(self, scale_factors)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03mSynchronise scales of the embeddings for each patch\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    scale_factors: if provided apply the given scales instead of synchronising\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_factors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     scale_factors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_synchronised_scales\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, scale \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(scale_factors):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatches[i]\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m scale\n",
      "File \u001b[0;32m~/Projects/G2007/code/L2GX/l2gx/align/alignment.py:184\u001b[0m, in \u001b[0;36mAlignmentProblem.calc_synchronised_scales\u001b[0;34m(self, max_scale)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mCompute the scaling transformations that best align the patches\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m scaling_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_matrix(\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m ov1, ov2: relative_scale(ov1, ov2, max_scale), \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    183\u001b[0m )\n\u001b[0;32m--> 184\u001b[0m vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_synchronise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaling_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m vec \u001b[38;5;241m=\u001b[39m vec\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    186\u001b[0m vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(vec)\n",
      "File \u001b[0;32m~/Projects/G2007/code/L2GX/l2gx/align/alignment.py:211\u001b[0m, in \u001b[0;36mAlignmentProblem._synchronise\u001b[0;34m(self, matrix, blocksize, symmetric)\u001b[0m\n\u001b[1;32m    199\u001b[0m     eigs, vecs \u001b[38;5;241m=\u001b[39m ss\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meigsh(\n\u001b[1;32m    200\u001b[0m         matrix,\n\u001b[1;32m    201\u001b[0m         k\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuckling\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# eigsh unreliable with multiple (clustered) eigenvalues, only buckling mode seems to help reliably\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# scaling is not symmetric but Perron-Frobenius applies\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     eigs, vecs \u001b[38;5;241m=\u001b[39m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     eigs \u001b[38;5;241m=\u001b[39m eigs\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m    213\u001b[0m     vecs \u001b[38;5;241m=\u001b[39m vecs\u001b[38;5;241m.\u001b[39mreal\n",
      "File \u001b[0;32m~/Projects/G2007/code/L2GX/.venv/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:1352\u001b[0m, in \u001b[0;36meigs\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, OPpart)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _ARPACK_LOCK:\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params\u001b[38;5;241m.\u001b[39mconverged:\n\u001b[0;32m-> 1352\u001b[0m         \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m params\u001b[38;5;241m.\u001b[39mextract(return_eigenvectors)\n",
      "File \u001b[0;32m~/Projects/G2007/code/L2GX/.venv/lib/python3.10/site-packages/scipy/sparse/linalg/_eigen/arpack/arpack.py:766\u001b[0m, in \u001b[0;36m_UnsymmetricArpackParams.iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_no_convergence()\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArpackError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo, infodict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate_infodict)\n",
      "\u001b[0;31mArpackError\u001b[0m: ARPACK error -9: Starting vector is zero."
     ]
    }
   ],
   "source": [
    "l2g_aligner = get_aligner(\"l2g\")\n",
    "l2g_aligner.align_patches(patches, scale=True)\n",
    "embedding = l2g_aligner.get_aligned_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter5'> üéØ <font color=\"grey\">Embedding </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2GX framework implements several graph embedding methods: ```SVDEmbedding```, ```GAEEmbedding```, ```VGAEEmbedding```, ```GraphSAGEEmbedding``` and ```DGIEmbedding```. The first three are based on transductive learning, while the last two are inductive.\n",
    "\n",
    "* <font color=\"grey\">SVD</font> - Classical spectral approach using eigendecomposition\n",
    "* <font color=\"grey\">GAE</font> - [Graph Auto-Encoder ](https://arxiv.org/abs/1611.07308) for deterministic reconstruction\n",
    "* <font color=\"grey\">VGAE</font> - [Variational Graph Auto-Encoder](https://arxiv.org/abs/1611.07308) with probabilistic latent variables\n",
    "* <font color=\"grey\">GraphSAGE</font> - [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) for scalable embedding\n",
    "* <font color=\"grey\">DGI</font> - [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) using self-supervised contrastive learning\n",
    "\n",
    "All methods are accessible through a unified interface with the ```get_embedding()``` function and registry system. The demonstration below shows convergence analysis, quality metrics, and UMAP visualizations for comprehensive comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.embedding import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìö Loading Cora dataset...\")\n",
    "cora_data = cora.to(\"torch-geometric\")\n",
    "print(f\"Cora dataset: {cora_data.num_nodes} nodes, {cora_data.num_edges} edges\")\n",
    "print(f\"Node features: {cora_data.x.shape}\")\n",
    "print(f\"Classes: {cora_data.y.unique().numel()}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_training_output(func, *args, **kwargs):\n",
    "    \"\"\"Capture stdout and parse training loss values.\"\"\"\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = captured_output = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        result = func(*args, **kwargs)\n",
    "        output = captured_output.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    # Parse loss values from output using regex\n",
    "    loss_pattern = r'Epoch\\s+(\\d+),\\s+Loss:\\s+([\\d.]+)'\n",
    "    matches = re.findall(loss_pattern, output)\n",
    "    \n",
    "    if matches:\n",
    "        epochs = [int(match[0]) for match in matches]\n",
    "        losses = [float(match[1]) for match in matches]\n",
    "        # Print the captured output so user can still see it\n",
    "        print(output, end='')\n",
    "        return result, (epochs, losses)\n",
    "    else:\n",
    "        print(output, end='')\n",
    "        return result, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "embedder = get_embedding('vgae', embedding_dim=64, num_epochs=200)\n",
    "embedding, loss_data = capture_training_output(embedder.fit_transform, cora_data)\n",
    "if loss_data:\n",
    "    epochs, losses = loss_data\n",
    "    training_history = {'epochs': epochs, 'losses': losses}\n",
    "else:\n",
    "    training_history = None\n",
    "end_time = time.time()\n",
    "embedding_time = end_time - start_time\n",
    "\n",
    "print(f\"VGAE embedding completed in {embedding_time:.2f}s\")\n",
    "print(f\"   Embedding shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"t-SNE VISUALIZATION OF EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "node_labels = cora_data.y.numpy()\n",
    "class_names = ['Case_Based', 'Genetic_Algorithms', 'Neural_Networks', \n",
    "               'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning', 'Theory']\n",
    "\n",
    "# Create t-SNE visualizations with optimal layout for 5 methods\n",
    "fig, axes = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "if torch.is_tensor(embedding):\n",
    "    embedding_np = embedding.detach().numpy().astype(np.float32)\n",
    "else:\n",
    "    embedding_np = np.array(embedding, dtype=np.float32)\n",
    "\n",
    "# Ensure data is finite and normalized\n",
    "embedding_np = np.nan_to_num(embedding_np, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction with robust parameters\n",
    "tsne_model = TSNE(\n",
    "    n_components=2, \n",
    "    random_state=42, \n",
    "    perplexity=min(30, len(embedding_np) - 1),  # Ensure perplexity < n_samples\n",
    "    n_iter=1000,\n",
    "    verbose=0,\n",
    "    init='pca',\n",
    "    learning_rate='auto'\n",
    ")\n",
    "\n",
    "embedding_2d = tsne_model.fit_transform(embedding_np)\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = axes  .scatter(embedding_2d[:, 0], embedding_2d[:, 1], \n",
    "                            c=node_labels, cmap='tab10', alpha=0.6, s=15)\n",
    "\n",
    "# Add method-specific styling\n",
    "axes.set_title(f'VGAE Embedding\\n(Time: {embedding_time:.2f}s)', fontsize=12)\n",
    "axes.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter6'> üîó <font color=\"grey\">Alignment </font></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l2gx.align import get_aligner\n",
    "from examples.example import (\n",
    "    generate_points, \n",
    "    voronoi_patches, \n",
    "    plot_patches, \n",
    "    transform_patches\n",
    ")\n",
    "from l2gx.align import procrustes_error\n",
    "rg = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = generate_points(n_clusters=5, scale=1.0, std=0.2, max_size=2000, min_size=128, dim=2)\n",
    "patches, centers = voronoi_patches(points, sample_size=10, min_degree=4, min_overlap=64, min_patch_size=128, eps=1, kmeans=False)\n",
    "overlap_count = 0\n",
    "for i in range(len(patches)):\n",
    "    for j in range(i+1, len(patches)):\n",
    "        overlap = len(set(patches[i].nodes) & set(patches[j].nodes))\n",
    "        if overlap >= 64:\n",
    "            overlap_count += 1\n",
    "\n",
    "print(f\"Patch pairs with sufficient overlap (‚â•64): {overlap_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0 # 0.1\n",
    "shift_scale = 10\n",
    "scale_range = None #[0.01, 100]\n",
    "\n",
    "# Create transformed copies of the patches\n",
    "transformed_patches = [copy.deepcopy(p) for p in patches]\n",
    "\n",
    "# Add noise to the transformed patches\n",
    "if noise_level > 0:\n",
    "    for patch in transformed_patches:\n",
    "        noise = rg.normal(loc=0, scale=noise_level, size=patch.coordinates.shape)\n",
    "        patch.coordinates += noise\n",
    "\n",
    "transformed_patches = transform_patches(\n",
    "    transformed_patches, \n",
    "    shift_scale=shift_scale, \n",
    "    scale_range=scale_range\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(patches, transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = get_aligner(\"geo2\", verbose=True, min_overlap=64)\n",
    "result = aligner.align_patches(transformed_patches, \n",
    "                      num_epochs=10000,\n",
    "                      learning_rate=0.01,\n",
    "                      use_orthogonal_reg=True,\n",
    "                      orthogonal_reg_weight=1000,\n",
    "                      use_bfs_training=True,\n",
    "                      center_patches=False,\n",
    "                      device=\"cpu\" \n",
    "                      )\n",
    "print(f\"Alignment completed. Final loss: {result.loss_history[-1]:.6f}\")\n",
    "aligned_embedding = result.get_aligned_embedding()\n",
    "\n",
    "_, _, procrustes_error = procrustes(points, aligned_embedding)\n",
    "print(f\"Procrustes error: {procrustes_error:.6f}\")\n",
    "\n",
    "plot_patches(patches, result.patches)\n",
    "\n",
    "intermediate_patches = copy.deepcopy(result.patches)\n",
    "result = aligner.align_patches(intermediate_patches, \n",
    "                      num_epochs=10000,\n",
    "                      learning_rate=0.01,\n",
    "                      use_orthogonal_reg=True,\n",
    "                      orthogonal_reg_weight=1000,\n",
    "                      use_bfs_training=True,\n",
    "                      center_patches=False,\n",
    "                      device=\"cpu\" \n",
    "                      )\n",
    "\n",
    "_, _, procrustes_error = procrustes(points, aligned_embedding)\n",
    "print(f\"Procrustes error: {procrustes_error:.6f}\")\n",
    "\n",
    "plot_patches(patches, result.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(patches, aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = generate_points(n_clusters=5, scale=1.0, std=0.2, max_size=2000, min_size=128, dim=2)\n",
    "patches, centers = voronoi_patches(points, sample_size=2, min_degree=1, min_overlap=64, min_patch_size=128, eps=1, kmeans=False)\n",
    "overlap_count = 0\n",
    "for i in range(len(patches)):\n",
    "    for j in range(i+1, len(patches)):\n",
    "        overlap = len(set(patches[i].nodes) & set(patches[j].nodes))\n",
    "        if overlap >= 64:\n",
    "            overlap_count += 1\n",
    "\n",
    "print(f\"Patch pairs with sufficient overlap (‚â•64): {overlap_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0 # 0.1\n",
    "shift_scale = 10\n",
    "scale_range = None #[0.01, 100]\n",
    "\n",
    "# Create transformed copies of the patches\n",
    "transformed_patches = [copy.deepcopy(p) for p in patches]\n",
    "\n",
    "# Add noise to the transformed patches\n",
    "if noise_level > 0:\n",
    "    for patch in transformed_patches:\n",
    "        noise = rg.normal(loc=0, scale=noise_level, size=patch.coordinates.shape)\n",
    "        patch.coordinates += noise\n",
    "\n",
    "transformed_patches = transform_patches(\n",
    "    transformed_patches, \n",
    "    shift_scale=shift_scale, \n",
    "    scale_range=scale_range\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_patches(patches, transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = get_aligner(\"geo2\", verbose=True, min_overlap=64)\n",
    "result = aligner.align_patches(transformed_patches, \n",
    "                      num_epochs=1000,\n",
    "                      learning_rate=0.01,\n",
    "                      use_orthogonal_reg=True,\n",
    "                      orthogonal_reg_weight=10,\n",
    "                      device=\"cpu\",\n",
    "                      center_patches=False\n",
    "                      )\n",
    "print(f\"Alignment completed. Final loss: {result.loss_history[-1]:.6f}\")\n",
    "aligned_embedding = result.get_aligned_embedding()\n",
    "\n",
    "_, _, procrustes_error = procrustes(points, aligned_embedding)\n",
    "print(f\"Procrustes error: {procrustes_error:.6f}\")\n",
    "\n",
    "plot_patches(patches, aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner = get_aligner(\n",
    "    \"geo\",\n",
    "    num_epochs=10000,\n",
    "    learning_rate=0.001,\n",
    "    model_type=\"affine\",\n",
    "    use_orthogonal_reg=True,\n",
    "    orthogonal_reg_weight=10,\n",
    "    center_patches=True,\n",
    "    preserve_scale=False,\n",
    "    min_overlap=64,\n",
    "    verbose=True)\n",
    "geo_aligner.align_patches(transformed_patches, min_overlap=64, scale=False)\n",
    "embedding = geo_aligner.get_aligned_embedding()\n",
    "plot_patches(patches, geo_aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procrustes_error(points, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner.debug_alignment_step_by_step(transformed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner.diagnose_alignment_issues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_aligner.check_overlap_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(geo_aligner.loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2g_aligner = get_aligner(\"l2g\")\n",
    "l2g_aligner.align_patches(transformed_patches, scale=True)\n",
    "embedding = l2g_aligner.get_aligned_embedding()\n",
    "plot_patches(patches, l2g_aligner.patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procrustes_error(points, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter7'> <font color=\"grey\">7. Hierarchical alignment </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='chapter8'> <font color=\"grey\">8. Visualisation </font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the visualisation, it is convenient to use external packages such as Heimdall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
